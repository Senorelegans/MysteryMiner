
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/5FilterTtest.ipynb
import json
import sys
import os
import os.path
from pathlib import Path
from subprocess import call
import subprocess
import sys # system libraries, like arguments (argv)
import re # regular expressions
import pandas as pd
import glob
import os.path
import numpy as np
from pathlib import Path


import scipy.stats as stats
# import researchpy as rp
import statsmodels.api as sm
from statsmodels.formula.api import ols


from scipy import stats


import statsmodels
# print(statsmodels.__version__)
# 0.8.0rc1

from statsmodels.sandbox.stats.multicomp import fdrcorrection0
# Pulled from https://pythonfordatascience.org/anova-python/



from timeit import default_timer as timer
import time



def removeFile(filename):
    try:
        os.remove(filename)
    except OSError:
        pass

def makeFolders(folder_list):
    for directory in folder_list:
        if not directory: # Make sure not an empty string ""
            continue
        else:
            if not os.path.exists(directory):
                os.makedirs(directory)

class FilterTtest():

    def __init__(self,
         NF_out,
         NF_path,
         col_data,
         title,
         control,
         treatment,
         p_adj,
         searchlist = None,
         binRank = None,
         min_length = 100):

        self.NF_path = NF_path
        self.p_adj = p_adj
        self.searchlist = searchlist
        self.binRank = binRank
        self.control = control
        self.treatment = treatment
        self.min_length = min_length

        self.title = title
        self.NF_out = NF_out
        self.wkdir = f'{self.NF_out}/unmapped/final'
        self.col_data = col_data
        self.df_col = pd.read_csv(self.col_data,sep="\t",names = ["sample","condition"])
        self.condition_list = self.df_col["condition"].unique()


    def searchDF(self, df, HAS, rank, name):
        df = df.fillna("NA")
        if HAS:
            df = df.loc[df[rank]==name]
        else:

            df = df.loc[df[rank]!=name]
        return df


    def Filter(self, df):
        if self.searchlist == None:
            return df
        else:
            for search in self.searchlist:
                if len(df) > 0:
                    HAS = search[0]
                    rank = search[1]
                    name = search[2]

                    print(f"Searching with {HAS}:{rank}:{name}")
                    df = self.searchDF(df=df, HAS=HAS, rank=rank,name=name)
            return df




    def runTtest(self,df):
        print(f"Running Ttest with {len(df)}")

        # Get coverages by conditions
        treatment_df = self.df_col[self.df_col["condition"]==self.treatment]
        treatment_list = list(treatment_df["sample"])
        control_df = self.df_col[self.df_col["condition"]==self.control]
        control_list = list(control_df["sample"])

        df_treatment = df[treatment_list]
        df_control = df[control_list]

        t_list = []
        p_val = []
        df["levine_statistic"] = np.nan
        df["levine_pval"] = np.nan
        df["t_statistic"] = np.nan
        df["t_pval"] = np.nan
#         df["t_padj"] = 0.0

        df = df.copy()
        for i in range(len(df)):
            treatment_vals = df_treatment.iloc[i,:]
            control_vals = df_control.iloc[i,:]

            lev = stats.levene(treatment_vals, control_vals)
            df["levine_statistic"].iloc[i] = lev[0]
            df["levine_pval"].iloc[i] = lev[1]


            ttest = stats.ttest_ind(treatment_vals, control_vals)
            df["t_statistic"].iloc[i] = ttest[0]
            df["t_pval"].iloc[i] = ttest[1]


        #Benjamini hochberg fdr correction
        df["t_padj"] = np.nan
        df_null = df[df["t_pval"].isnull()]
        df = df[~df["t_pval"].isnull()] # Get everything not null because statsmodel cant handle it

        padj = statsmodels.sandbox.stats.multicomp.fdrcorrection0(df["t_pval"], alpha=0.05, method='indep', is_sorted=False)

        df["t_padj_reject"] = padj[0]
        df["t_padj"] = list(padj[1])
        df = pd.concat([df,df_null], sort=False)

#         print(df)


        cols = list(df)

        statcols = ["contig", "name", "superkingdom", "kingdom", "phylum", "order",	"family", "genus", "species", "blast_pident", "blast_sseqid", "blast_evalue", "taxid", "blast_stitle", "filename", "length", "fasta"]
        statcols = ["levine_statistic","levine_pval","t_statistic","t_pval","t_padj","t_padj_reject"]
        newcols = [x for x in cols if x not in statcols]
        cols = statcols + newcols
        df = df[cols]


        return df

    def runSearch(self):
        files = glob.glob(f'{self.wkdir}/pileupCoverageNormalizedMatched/*.txt')

        out = f'{self.NF_out}/FINAL_OUT/{self.title}'
        ddout = f'{self.NF_out}/FINAL_OUT/{control}_{treatment}_doubledark'
        minout = f'{out}/minlength_{self.min_length}'
        makeFolders([f'{self.NF_out}/FINAL_OUT/',out, ddout])

        empties = []


            #Run T test for no hits if it doesn't exist
        if os.path.exists(f'{ddout}/{title}_dark_all_FG_NoHits.txt') == False:
            df = pd.read_csv(f'{self.wkdir}/pileupCoverageNormalizedMatched/dark_all_FG_NoHits_pileupCoverageNormalizedMatched.tsv',sep="\t")
            df = self.runTtest(df)
            df_sig = df[df["t_padj_reject"]==True]
            f_sig = f'{ddout}/{control}_{treatment}_doubledark_double_dark_all_FG_NoHits_Significant.txt'
            f_df = f'{ddout}/{control}_{treatment}_doubledark_double_dark_all_FG_NoHits.txt'
            df_sig.to_csv(f_sig,sep="\t",index=None)
            df.to_csv(f_df,sep="\t",index=None)




        dfs = []
        for f in files:
            print("---------------")
            file = os.path.basename(f)
            print("out:", out)
            print(f' Working on file {file}')
            df = pd.read_csv(f,sep="\t",low_memory=False)
            print(f"Starting contigs: {len(df)}")
            df = self.Filter(df)


            if len(df) > 0:
                if self.binRank != None:
                    df = df.groupby([self.binRank]).sum()
                    print(f"Binning by Rank {self.binRank}: Length: {len(df)}")



#                 print(df.head())
                df = self.runTtest(df)
#                 print(df.head())
                df_sig = df[df["t_padj_reject"]==True]
                f_sig = f'{out}/{self.title}_Significant_{file}'
                f_df = f'{out}/{self.title}_{file}'

                self.df_sig = df_sig
                if self.binRank != None:
                    df_sig.to_csv(f_sig,sep="\t")
                    df.to_csv(f_df,sep="\t")
                else:
                    df_sig.to_csv(f_sig,sep="\t",index=None)
                    df.to_csv(f_df,sep="\t",index=None)


            else:

                empties.append(file)

#         if len(empties)  > 0:

#             with open(f'{minout}/Empty_files.txt',"w") as outfile:
#                 for empty in empties:
#                     outfile.write(os.path.basename(empty)+"\n")



import argparse
def parse_arguments():
        parser = argparse.ArgumentParser(description='filter quantify and graph')
        parser.add_argument('--Nextflow_Out', action= 'store', metavar='--Nextflow_Out')
        parser.add_argument('--Nextflow_path', action= 'store', metavar='--Nextflow_path')
        args = parser.parse_args()
        return args


if __name__=="__main__":
    args = parse_arguments()

    NF_out = args.Nextflow_Out
    NF_path = args.Nextflow_path
    col_data = f"{NF_path}/sample_table.txt"


    levels = ["all","dark_all_FG"]

    ranks = ['superkingdom',
 'kingdom',
 'phylum',
 'order',
 'family',
 'genus',
 'species',"name"]


    superkingdoms = ['Bacteria', 'Viruses', 'Eukaryota', 'NA', 'Archaea']


    # Make permutation of conditions
    col_data = f"{NF_path}/sample_table.txt"
    df_col = pd.read_csv(col_data,sep="\t",names=["sample","condition"])
    items = list(df_col["condition"].unique())
    conditions = [(items[i],items[j]) for i in range(len(items)) for j in range(i+1, len(items))]





    # This will run all contigs
    searchlist = None
    binRank = None



    for permutation in conditions:
        control = permutation[0]
        treatment = permutation[1]

        title = f"{control}_{treatment}_OUT_ALL_CONTIGS"

        T = FilterTtest(NF_out=NF_out,
                     NF_path=NF_path,
                     col_data = col_data,
                     title=title,
                     searchlist=searchlist,
                     control = control,
                     treatment = treatment,
                     binRank = binRank,
                     min_length = 100,
                     p_adj = 0.05
                    )

        T.runSearch()



    for permutation in conditions:
        control = permutation[0]
        treatment = permutation[1]

        for superkingdom in superkingdoms:
            print(f"**** working on {superkingdom}")
            searchlist = [(True,"superkingdom",superkingdom)]
            for binRank in ranks:

                makeFolders(f"OUT_{superkingdom}")
                title = f"{control}_{treatment}_{superkingdom}_Bin_{binRank}"

                T = FilterTtest(NF_out=NF_out,
                             NF_path=NF_path,
                             col_data = col_data,
                             title=title,
                             searchlist=searchlist,
                             control = control,
                             treatment = treatment,
                             binRank = binRank,
                             min_length = 100,
                             p_adj = 0.05
                            )

                T.runSearch()
