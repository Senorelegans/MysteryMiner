{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:19:12.971827Z",
     "start_time": "2020-01-17T16:19:12.952538Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import sys # system libraries, like arguments (argv)\n",
    "import re # regular expressions\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import statsmodels as sm\n",
    "\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:19:14.007715Z",
     "start_time": "2020-01-17T16:19:14.003539Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def removeFile(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:19:14.319104Z",
     "start_time": "2020-01-17T16:19:14.313658Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def makeFolders(folder_list):\n",
    "    for directory in folder_list:\n",
    "        if not directory: # Make sure not an empty string \"\"\n",
    "            continue\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter class (Filter contigs of interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:19:16.599339Z",
     "start_time": "2020-01-17T16:19:16.590936Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class JSON():\n",
    "    def __init__(self, f, explicit, min_length = 100):\n",
    "        self.f = f\n",
    "        self.min_length = min_length\n",
    "        self.json_empty = False\n",
    "        self.num_contigs = 0\n",
    "        with open(f) as fo:\n",
    "            cd = json.load(fo) # cd is contig dictionary\n",
    "            k = list(cd.keys())\n",
    "            print(f'starting with {len(k)} contigs')\n",
    "            if len(k) > 0:\n",
    "                self.filename = cd[k[0]][\"filename\"]\n",
    "                self.cd = cd\n",
    "            else:\n",
    "                self.json_empty = True\n",
    "                self.cd = {}\n",
    "                self.pe(f'*-**-**-**-**-**-**-**-**-**-* WARNING file Empty!!!')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Jsons after query and make gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:28:24.066666Z",
     "start_time": "2020-01-17T16:28:24.037093Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Match():\n",
    "    \n",
    "    def __init__(self, \n",
    "             NF_out, \n",
    "             NF_path,\n",
    "             col_data, \n",
    "             levels,\n",
    "             explicit = True):\n",
    "\n",
    "        self.explicit = explicit\n",
    "        self.NF_out = NF_out\n",
    "        self.NF_path = NF_path\n",
    "        self.wkdir = f'{self.NF_out}/unmapped/final'\n",
    "        self.Jlist_d = {}\n",
    "        self.df_norm = pd.DataFrame()\n",
    "        self.col_data = col_data\n",
    "        self.df_col = pd.read_csv(self.col_data,sep=\"\\t\",names = [\"sample\",\"condition\"])\n",
    "        self.condition_list = self.df_col[\"condition\"].unique()\n",
    "#         self.levels = [\"single\",\"group\",\"all\",\n",
    "#                        \"dark_single_FG\",\"dark_group_FA\",\"dark_all_FG\"]\n",
    "        self.levels = levels\n",
    "        self.toJSON()\n",
    "\n",
    "\n",
    "\n",
    "    def toJSON(self):\n",
    "        import ast\n",
    "        \n",
    "\n",
    "        for f in glob.glob(self.wkdir+\"/jgi_df/*/*txt\"):\n",
    "#             print(f)\n",
    "            \n",
    "            l = len(\"_df.txt\")\n",
    "            name = os.path.basename(f)[:-l]\n",
    "            fout = f[:-l]\n",
    "            df = pd.read_csv(f,sep=\"\\t\")\n",
    "#             print(df.head())\n",
    "            df_J = df[[\"taxid\",\"jgi_json\"]]\n",
    "            del df[\"jgi_json\"]\n",
    "            df.to_json(f\"{fout}_temp.json\",orient='index')\n",
    "            \n",
    "            with open(f\"{fout}_temp.json\") as fo:\n",
    "                contig_d = json.load(fo)\n",
    "                for c in contig_d.keys():\n",
    "                    d = contig_d[c] \n",
    "                    taxid = d[\"taxid\"]  \n",
    "                    df_temp = df_J[df_J[\"taxid\"]==taxid]\n",
    "                    j = df_temp[\"jgi_json\"].iloc[0]\n",
    "                    jdic = ast.literal_eval(j)\n",
    "                    d[\"jgi_json\"] = jdic\n",
    "                    d[\"filename\"] = name\n",
    "\n",
    "                with open(f\"{fout}.json\", \"w\") as write_file:\n",
    "                    json.dump(contig_d, write_file)\n",
    "            removeFile(f\"{fout}_temp.json\")\n",
    "  \n",
    "        \n",
    "    \n",
    "              \n",
    "\n",
    "    def makeDF(self,cd, level):\n",
    "        print(\"Making df\")\n",
    "        # Add NA stuff to it\n",
    "        for contig_key in cd:\n",
    "            contig = cd[contig_key]\n",
    "            contigname = contig[\"contig\"]\n",
    "            jgi = contig[\"jgi_json\"]\n",
    "            try:\n",
    "                superkingdom = jgi[\"superkingdom\"]\n",
    "            except:\n",
    "                jgi[\"superkingdom\"] = {}\n",
    "                jgi[\"superkingdom\"][\"name\"] = \"NA\"\n",
    "            try:\n",
    "                kingdom = jgi[\"kingdom\"]\n",
    "            except:\n",
    "                jgi[\"kingdom\"] = {}\n",
    "                jgi[\"kingdom\"][\"name\"] = \"NA\"\n",
    "            try:\n",
    "                phylum = jgi[\"phylum\"]\n",
    "            except:\n",
    "                jgi[\"phylum\"] = {}\n",
    "                jgi[\"phylum\"][\"name\"] = \"NA\" \n",
    "            try:\n",
    "                order = jgi[\"order\"]\n",
    "            except:\n",
    "                jgi[\"order\"] = {}\n",
    "                jgi[\"order\"][\"name\"] = \"NA\"    \n",
    "            try:\n",
    "                family = jgi[\"family\"]\n",
    "            except:\n",
    "                jgi[\"family\"] = {}\n",
    "                jgi[\"family\"][\"name\"] = \"NA\"    \n",
    "            try:\n",
    "                genus = jgi[\"genus\"]\n",
    "            except:\n",
    "                jgi[\"genus\"] = {}\n",
    "                jgi[\"genus\"][\"name\"] = \"NA\" \n",
    "            try:\n",
    "                species = jgi[\"species\"]\n",
    "            except:\n",
    "                jgi[\"species\"] = {}\n",
    "                jgi[\"species\"][\"name\"] = \"NA\"                 \n",
    "\n",
    "                # Update the contig list\n",
    "        cd[contig_key][\"jgi_json\"] = jgi\n",
    "     # Add to dataframe and write out\n",
    "        df = pd.DataFrame.from_dict(cd).T\n",
    "\n",
    "        if level == \"all\":\n",
    "            df = df[[\"contig\",\"blast_pident\",\"blast_sseqid\", \"blast_evalue\" ,\"taxid\" ,\"blast_stitle\",\"filename\",\"length\",\"fasta\"]]\n",
    "        else:\n",
    "            df = df[[\"contig\",\"taxid\" ,\"filename\",\"length\",\"fasta\"]]\n",
    "        \n",
    "        df_final = pd.DataFrame()\n",
    "        for contig_key in cd:\n",
    "            contig = cd[contig_key]\n",
    "            contigname = contig[\"contig\"]\n",
    "            jgi = contig[\"jgi_json\"]\n",
    "            dfJ = pd.DataFrame.from_dict(jgi)\n",
    "\n",
    "            dfJ[\"contig\"] = contigname\n",
    "            dfJ = dfJ[[\"contig\",\"name\",\"superkingdom\",\"kingdom\",\"phylum\",\"order\",\"family\",\"genus\",\"species\"]]\n",
    "            dfJ = dfJ.loc[[\"name\"]]\n",
    "\n",
    "\n",
    "            df_final = pd.concat([df_final,dfJ])\n",
    "        df = pd.merge(df_final,df,on=\"contig\")       \n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "    def writeDF(self):\n",
    "#         makeFolders([f'{self.NF_out}/{self.title}'])\n",
    "        for level in self.levels:\n",
    "            f_pileup = \"\"\n",
    "            if level == \"all\" or \"dark_all_FG\":\n",
    "                f_pileup = f\"{self.wkdir}/pileupCoverageNormalized/all_pileupCoverageNormalized.txt\"\n",
    "            \n",
    "\n",
    "            \n",
    "            df_pileup = pd.read_csv(f_pileup,sep=\"\\t\")\n",
    "\n",
    "            df_final = pd.DataFrame()\n",
    "\n",
    "            for J in self.Jlist_d[level]:\n",
    "                df = self.makeDF(J.cd, level)\n",
    "#                 df = pd.DataFrame.from_dict(J.cd).T\n",
    "\n",
    "                df_final = pd.concat([df_final,df])\n",
    "                \n",
    "            df_final = pd.merge(df_final,df_pileup,on=\"contig\")\n",
    "            \n",
    "            out = f\"{self.wkdir}/pileupCoverageNormalizedMatched\"\n",
    "            makeFolders([out])\n",
    "            f_df = f'{out}/{level}_pileupCoverageNormalizedMatched.txt'\n",
    "\n",
    "            df_final.to_csv(f_df,sep=\"\\t\",index=None)\n",
    "            \n",
    "            \n",
    "          \n",
    "            if level == \"dark_all_FG\":\n",
    "# /Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/Z_RNAseq-Biome-master/NF_OUT/unmapped/final/darkbiome/lastdb/FINAL_Nohits/all_group/all/all_darkbiome_df.txt                \n",
    "                f_df = f'{out}/{level}_NoHits_pileupCoverageNormalizedMatched.tsv'\n",
    "                df_final = pd.read_csv(f\"{self.wkdir}/darkbiome/lastdb/FINAL_Nohits/all_group/all/all_darkbiome_df.txt\",sep=\"\\t\")\n",
    "\n",
    "                df_final = pd.merge(df_final,df_pileup,on=\"contig\")  \n",
    "                df_final.to_csv(f_df,sep=\"\\t\",index=None)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:21:42.535140Z",
     "start_time": "2020-01-17T17:21:41.985007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on all --- \n",
      "*_*_*_*_*_*_*_ WARNING *_*_*_*_*_*_*_\n",
      "No files in all had any contigs\n",
      "Working on dark_all_FG --- \n",
      "*_*_*_*_*_*_*_ WARNING *_*_*_*_*_*_*_\n",
      "No files in dark_all_FG had any contigs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/trem2/RNAseq-Biome-master/NF_OUT/unmapped/final/pileupCoverageNormalized/all_pileupCoverageNormalized.txt' does not exist: b'/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/trem2/RNAseq-Biome-master/NF_OUT/unmapped/final/pileupCoverageNormalized/all_pileupCoverageNormalized.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1f1c4bbed2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchContigs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-8e664744907f>\u001b[0m in \u001b[0;36mwriteDF\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mdf_pileup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_pileup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mdf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/trem2/RNAseq-Biome-master/NF_OUT/unmapped/final/pileupCoverageNormalized/all_pileupCoverageNormalized.txt' does not exist: b'/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/trem2/RNAseq-Biome-master/NF_OUT/unmapped/final/pileupCoverageNormalized/all_pileupCoverageNormalized.txt'"
     ]
    }
   ],
   "source": [
    "NF = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/nextflow/RNAseq-Biome-Nextflow\"\n",
    "\n",
    "NF = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/Z_RNAseq-Biome-master\"\n",
    "\n",
    "\n",
    "NF = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/trem2/RNAseq-Biome-master\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NF_out = f\"{NF}/NF_OUT\" \n",
    "NF_path = NF\n",
    "out_path = NF\n",
    "col_data = f\"{NF}/sample_table.txt\"\n",
    "levels = [\"all\",\"dark_all_FG\"]\n",
    "# levels = [\"dark_all_FG\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "M = Match(NF_out=NF_out, \n",
    "             NF_path=NF_path,\n",
    "             col_data = col_data, \n",
    "             levels = levels,\n",
    "             explicit = True\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "M.searchContigs()\n",
    "M.writeDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:28:27.702904Z",
     "start_time": "2020-01-17T16:28:27.699429Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import argparse\n",
    "def parse_arguments():\n",
    "        parser = argparse.ArgumentParser(description='filter quantify and graph')\n",
    "        parser.add_argument('--Nextflow_Out', action= 'store', metavar='--Nextflow_Out') \n",
    "        parser.add_argument('--Nextflow_path', action= 'store', metavar='--Nextflow_path')\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T16:28:29.107032Z",
     "start_time": "2020-01-17T16:28:29.100453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating pileup into dataframes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--Nextflow_Out --Nextflow_Out]\n",
      "                             [--Nextflow_path --Nextflow_path]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/m/Library/Jupyter/runtime/kernel-28df6773-ac50-4ebb-a5cc-87b6c810090f.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Concatenating pileup into dataframes...\")\n",
    "    args = parse_arguments()\n",
    "\n",
    "    \n",
    "  \n",
    "    NF_out = args.Nextflow_Out\n",
    "    NF_path = args.Nextflow_path\n",
    "    col_data = f\"{NF_path}/sample_table.txt\"\n",
    "\n",
    "    \n",
    "    levels = [\"all\",\"dark_all_FG\"]\n",
    "    M = Match(NF_out=NF_out, \n",
    "                 NF_path=NF_path,\n",
    "                 col_data = col_data, \n",
    "                 levels = levels,\n",
    "                )\n",
    "\n",
    "\n",
    "    M.searchContigs()\n",
    "    M.writeDF()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T17:25:51.034522Z",
     "start_time": "2020-01-17T17:25:50.773283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 3.0-PileupDataFrame.ipynb to nb_3.0-PileupDataFrame.py\r\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "!python notebook2script.py 3.0-PileupDataFrame.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
