{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:34.265639Z",
     "start_time": "2020-09-19T21:55:34.260908Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import sys # system libraries, like arguments (argv)\n",
    "import re # regular expressions\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:34.426064Z",
     "start_time": "2020-09-19T21:55:34.422380Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def makeFolders(folder_list):\n",
    "    for directory in folder_list:\n",
    "        if not directory: # Make sure not an empty string \"\"\n",
    "            continue\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:34.716140Z",
     "start_time": "2020-09-19T21:55:34.713503Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def removeFile(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError:\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build contig fastas, make Bowtie2 indexes, Map to those indexes.\n",
    "## for Single (S), Group (G), All (A), Filtered (SG) (SA) (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.052967Z",
     "start_time": "2020-09-19T21:55:35.050323Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def makeColDF(col_data):\n",
    "    df = pd.read_csv(col_data,sep=\"\\t\",names = [\"sample\",\"condition\"])\n",
    "    condition_list = df[\"condition\"].unique()\n",
    "    return df, condition_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.223232Z",
     "start_time": "2020-09-19T21:55:35.216808Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "col_data = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/nextflow/RNAseq-Biome-Nextflow/sample_table.txt\"\n",
    "df_col, condition_list = makeColDF(col_data=col_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.429797Z",
     "start_time": "2020-09-19T21:55:35.397778Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class darkLASTDB():\n",
    "    def __init__(self, NF_out, col_data, nr_db, cpus=4,percent_identity=60.0):\n",
    "        self.NF_out = NF_out\n",
    "        self.wkdir = f\"{self.NF_out}/unmapped/final/darkbiome\"\n",
    "        self.cpus = cpus\n",
    "        self.col_data = col_data\n",
    "        self.percent_identity = percent_identity\n",
    "        self.single = []\n",
    "        self.group = []\n",
    "        self.group_FS = []\n",
    "        self.all = []\n",
    "        self.grab_list = []\n",
    "        self.grab_list_name = []\n",
    "        self.nr_db = nr_db\n",
    "        self.topHeir = False\n",
    "\n",
    "    def darkmakeColDF(self):\n",
    "        self.df_col = pd.read_csv(self.col_data,sep=\"\\t\",names = [\"sample\",\"condition\"])\n",
    "        self.cl = self.df_col[\"condition\"].unique()\n",
    "    \n",
    "    def darklastIndex(self):\n",
    "        makeFolders([f'{self.wkdir}/lastdb',f'{self.wkdir}/lastdb/lastdb_index'])\n",
    "\n",
    "        for c in self.cl:\n",
    "            index_out = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_{c}'\n",
    "            makeFolders([index_out])\n",
    "            fa = f'{self.wkdir}/afterNext/group/group_{c}_darkbiome.fasta'\n",
    "            self.removeFaDup(fa)\n",
    "            index_cmd = f'lastdb -cR01 {index_out}/group_{c} {fa}'\n",
    "            subprocess.run(index_cmd, shell=True) \n",
    "            self.group.append(fa)\n",
    "\n",
    "        index_out = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_all'\n",
    "        makeFolders([index_out])\n",
    "        fa = f'{self.wkdir}/afterNext/all/all_darkbiome.fasta'\n",
    "        index_cmd = f'lastdb -cR01 {index_out}/all {fa}'\n",
    "        subprocess.run(index_cmd, shell=True)     \n",
    "        self.all.append(fa)\n",
    "      \n",
    "\n",
    "    def darkrunLast(self):\n",
    "        f_in = f'{self.wkdir}/afterNext/single/' \n",
    "        out = f'{self.wkdir}/lastdb/lastdb_out/group_single'\n",
    "        makeFolders([f'{self.wkdir}/lastdb/lastdb_out/',out])\n",
    "        self.single = []\n",
    "        \n",
    "        if self.topHeir == False:\n",
    "            for x in range(len(self.df_col)):\n",
    "                c = self.df_col[\"condition\"].iloc[x]\n",
    "                name = self.df_col[\"sample\"].iloc[x]\n",
    "                fa = f_in+name+\"_darkbiome.fasta\"\n",
    "                self.removeFaDup(fa)\n",
    "                index = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_{c}/group_{c}'\n",
    "                last_cmd = f'lastal -f BlastTab+ {index} {fa} > {out}/{name}.tab'\n",
    "                subprocess.run(last_cmd, shell=True)\n",
    "                self.single.append(fa)\n",
    "            \n",
    "            \n",
    "        if self.topHeir == True:\n",
    "            #Map condition to all\n",
    "            out = f'{self.wkdir}/lastdb/lastdb_out/all_group'\n",
    "            makeFolders([out])        \n",
    "            for c in self.cl:\n",
    "                fa = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/group/group_{c}_darkbiome.fasta'\n",
    "                name = f'group_{c}'\n",
    "                index = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_all/all'\n",
    "                last_cmd = f'lastal -f BlastTab+ {index} {fa} > {out}/{name}.tab'\n",
    "                subprocess.run(last_cmd, shell=True)\n",
    "                self.group_FS.append(fa)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def darkcleanLast(self, tab_path, query, subject): \n",
    "        #query is lower on totem pole\n",
    "        df = pd.DataFrame()\n",
    "        for f in glob.glob(tab_path+\"/*.tab\"):\n",
    "            print(f)\n",
    "            df_t = pd.read_csv(f, sep=\"\\t\", skiprows=26,\n",
    "                              names=[\"query_id\",\"subject_id\",\"percent_identity\",\n",
    "                                     \"alignment_length\",\"mismatches\",\"gap_opens\",\n",
    "                                     \"q_start\",\"q_end\",\"s_start\",\"s_end\",\"evalue\",\n",
    "                                     \"bit_score\",\"query_length\",\"subject_length\",\n",
    "                                     \"raw_score\"])\n",
    "            df_t=df_t[~df_t[\"query_id\"].str.contains(\"#\")]\n",
    "            print(df)\n",
    "            df = pd.concat([df,df_t])\n",
    "            \n",
    "        print(df)\n",
    "        df = df[df[\"percent_identity\"]>self.percent_identity]\n",
    "        q_df = df[[\"query_id\"]]\n",
    "        s_df = df[[\"subject_id\"]]\n",
    "        q_df = q_df.rename(columns={\"query_id\":\"contig\"})\n",
    "        s_df = s_df.rename(columns={\"subject_id\":\"contig\"})\n",
    "        q_df = q_df.drop_duplicates(subset=\"contig\")\n",
    "        s_df = s_df.drop_duplicates(subset=\"contig\")       \n",
    "            \n",
    "        out = f'{self.wkdir}/lastdb/lastdb_cleaned/{query}_{subject}'\n",
    "        makeFolders([f'{self.wkdir}/lastdb/lastdb_cleaned/',out]) \n",
    "        q_df.to_csv(f'{out}/{query}_filteredBy_{subject}.txt',sep=\"\\t\", index=None, header=None)\n",
    "        s_df.to_csv(f'{out}/{subject}_filteredBy_{query}.txt',sep=\"\\t\", index=None, header=None)\n",
    "        df_grab = pd.concat([q_df,s_df])\n",
    "        df_grab.to_csv(f'{out}/grab_{subject}_{query}.txt',sep=\"\\t\",\n",
    "                           index=None, header=None)\n",
    "        self.grab_list += [df_grab]\n",
    "        self.grab_list_name += [f'{out}/grab_{subject}_{query}.txt']\n",
    "\n",
    "        \n",
    "    def darkfilterCleaned(self, out, fasta_list, grab):\n",
    "        for f in fasta_list:\n",
    "            name = os.path.basename(f)\n",
    "            cmd  = f'seqtk subseq {f} {grab} > {out}/{name}'\n",
    "            subprocess.run(cmd, shell=True)                  \n",
    "   \n",
    "    def darkruncleanLast(self):\n",
    "        for data in [\"json\",\"df\",\"fasta\"]:\n",
    "            makeFolders([f'{self.wkdir}/lastdb/lastdb_{data}/',\n",
    "                     f'{self.wkdir}/lastdb/lastdb_{data}/group_single',\n",
    "                     f'{self.wkdir}/lastdb/lastdb_{data}/group_single/single',\n",
    "                     f'{self.wkdir}/lastdb/lastdb_{data}/group_single/group',\n",
    "                     f'{self.wkdir}/lastdb/lastdb_{data}/all_group',\n",
    "                     f'{self.wkdir}/lastdb/lastdb_{data}/all_group/all',\n",
    "                     f'{self.wkdir}/lastdb/lastdb_{data}/all_group/group',                     \n",
    "                    ])   \n",
    "            \n",
    "            \n",
    "        #Clean singles and groups\n",
    "        self.darkrunLast()\n",
    "        self.darkcleanLast(tab_path=f'{self.wkdir}/lastdb/lastdb_out/group_single', \n",
    "                      query=\"single\", subject=\"group\")\n",
    "        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/single',\n",
    "                      fasta_list = self.single, \n",
    "                      grab = self.grab_list_name[0])\n",
    "        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/group',\n",
    "                      fasta_list = self.group, \n",
    "                      grab = self.grab_list_name[0]) \n",
    "        \n",
    "        \n",
    " \n",
    "        self.topHeir = True # Change this to true so you can start function below\n",
    "        self.darkrunLast()      \n",
    "        #Clean group filtered by single and all\n",
    "        self.darkcleanLast(tab_path=f'{self.wkdir}/lastdb/lastdb_out/all_group', \n",
    "                       query=\"group\", subject=\"all\")\n",
    "        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/group',\n",
    "                      fasta_list = self.group_FS, \n",
    "                      grab = self.grab_list_name[1])\n",
    "        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/all',\n",
    "                      fasta_list = self.all, \n",
    "                      grab = self.grab_list_name[1])           \n",
    "        \n",
    "\n",
    "    def removeFaDup(self, f1):\n",
    "        tmp = f1+\"tmp\"\n",
    "        with open(f1, \"r\") as infile, open(tmp,\"w\") as outfile:\n",
    "            namelist = []\n",
    "            WRITE = True\n",
    "            for line in infile:\n",
    "                if \">\" in line:                \n",
    "                    name = line\n",
    "                    if name in namelist:\n",
    "                        WRITE = False\n",
    "                    else: \n",
    "                        WRITE = True\n",
    "                    namelist.append(name)\n",
    "                if WRITE:\n",
    "                    outfile.write(line)\n",
    "\n",
    "        with open(tmp,\"r\") as infile, open(f1,\"w\") as outfile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "        removeFile(tmp)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def darkblastX(self, path,out): \n",
    "        for fa in glob.glob(path+\"/*.fasta\"):\n",
    "            l = len(\".fasta\")\n",
    "            name = f'{os.path.basename(fa)[:-l]}'\n",
    "            self.removeFaDup(fa)\n",
    "            cmd  = f'blastx -db {self.nr_db} -query {fa} -max_target_seqs 10 -outfmt 15 -num_threads {cpus} -out {out}/{name}.json'\n",
    "            subprocess.run(cmd, shell=True)         \n",
    "\n",
    "    def rundarkblastX(self):\n",
    "#         self.darkblastX(path = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/single',\n",
    "#                 out = f'{self.wkdir}/lastdb/lastdb_json/group_single/single')   \n",
    "#         self.darkblastX(path = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/group',\n",
    "#                 out = f'{self.wkdir}/lastdb/lastdb_json/all_group/group') \n",
    "        self.darkblastX(path = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/all',\n",
    "                out = f'{self.wkdir}/lastdb/lastdb_json/all_group/all')     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.546177Z",
     "start_time": "2020-09-19T21:55:35.544169Z"
    }
   },
   "outputs": [],
   "source": [
    "# NF_out = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/Z_RNAseq-Biome-master/NF_OUT\"\n",
    "# cpus = 4 \n",
    "# F = darkFilterJGI(NF_out=NF_out, cpus=cpus)\n",
    "# F.tophit()\n",
    "# nr_db=\"\"\n",
    "\n",
    "# L = darkLASTDB(NF_out=NF_out, col_data=col_data, cpus=cpus, nr_db=nr_db)\n",
    "# L.darkmakeColDF()\n",
    "# L.darklastIndex()\n",
    "# L.darkruncleanLast() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.711308Z",
     "start_time": "2020-09-19T21:55:35.678834Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class darkFilterJGI():\n",
    "    def __init__(self, NF_out, cpus=4):\n",
    "        self.NF_out = NF_out\n",
    "        self.wkdir = f'{self.NF_out}/unmapped/final/darkbiome/lastdb'\n",
    "        self.cpus = cpus\n",
    "        \n",
    "        \n",
    "    def matchFasta(self, df, fasta, fout, nohits):\n",
    "        c11 = fout+\"_newfasta1.fast\"\n",
    "        c12 = fout+\"_newfasta2.fast\"\n",
    "        with open(fasta,'r', newline=\"\\n\") as infile, open(c11, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                if \">\" in line:\n",
    "                    line = line.replace('\\n', '\\t')\n",
    "                    line = line.replace('>', '\\n')\n",
    "                    outfile.write(line)\n",
    "                else:\n",
    "                    line = line.replace('\\n', 'RTN666')\n",
    "                    outfile.write(line)\n",
    "            infile.close()\n",
    "            outfile.close()\n",
    "\n",
    "        with open(c11,'r', newline=\"\\n\") as infile, open(c12, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                if \"NODE\" not in line:\n",
    "                    line = line.replace('\\n', '')\n",
    "                    line = line.replace('\\r', '')\n",
    "                    outfile.write(line)\n",
    "                else:\n",
    "                    outfile.write(line)\n",
    "            infile.close()\n",
    "            outfile.close()\n",
    "\n",
    "        my_cols=['contig', 'fasta_with_returnsAdded']\n",
    "        df_fa=pd.read_csv(c12, index_col=['contig'], sep='\\t', names=my_cols)\n",
    "        df = pd.merge(df, df_fa, on='contig') # Dont run this cell more than once\n",
    "        df[\"fasta\"] = df['fasta_with_returnsAdded'].str.replace(\"RTN666\",\"\")\n",
    "        df[\"length\"] = df[\"fasta\"].str.len()\n",
    "        \n",
    "        # Write out fasta\n",
    "        with open(fout+\".fasta\",\"w\") as outfile:\n",
    "            for x in range(len(df)):\n",
    "                nodename = \">\"+df[\"contig\"].iloc[x]+\"\\n\"\n",
    "                fa = df['fasta_with_returnsAdded'].iloc[x]\n",
    "                fa = fa.replace('RTN666', '\\n')\n",
    "                outfile.write(nodename+fa)        \n",
    "\n",
    "        if nohits:\n",
    "            df = df[[\"contig\",\"length\",\"fasta\"]]\n",
    "        else:\n",
    "            df = df[[\"contig\",\"taxid\",\"length\",\"jgi_tab\",\"jgi_json\",\n",
    "                    \"fasta\"]]\n",
    "        df.to_csv(fout+\"_df.txt\",sep=\"\\t\",index=None)\n",
    "        removeFile(c11)\n",
    "        removeFile(c12) \n",
    "        return df\n",
    "\n",
    "\n",
    "    def JGIQuery(self, df, garbage_out):\n",
    "        garbage_out = garbage_out[:-4]\n",
    "        df = df.copy()\n",
    "        taxids=df['taxid'].unique()\n",
    "        df_ids = pd.DataFrame(taxids) \n",
    "        df_ids = df_ids.rename(columns = {0:\"taxid\"})\n",
    "\n",
    "        df_ids[\"jgi_tab\"] = \"NA\"\n",
    "        df_ids[\"jgi_json\"] = \"NA\"\n",
    "        i = 0\n",
    "        R = len(df_ids)\n",
    "        for x in range(R):\n",
    "            print(f'Querying {i+1} of {R} taxids')\n",
    "            id = df_ids[\"taxid\"].iloc[x]\n",
    "            Q1 = \"curl https://taxonomy.jgi-psf.org/sc/id/\" + str(id)\n",
    "            result = os.popen(Q1).read()\n",
    "            df_ids[\"jgi_tab\"].iloc[x] = result\n",
    "\n",
    "            #Get json return\n",
    "            jasonQ1 = \"curl https://taxonomy.jgi-psf.org/id/\" + str(id)\n",
    "            result = os.popen(jasonQ1).read()\n",
    "            j = json.loads(result)\n",
    "            j = j[list(j.keys())[0]]\n",
    "            j = str(j)\n",
    "            df_ids[\"jgi_json\"].iloc[x] = j  \n",
    "\n",
    "            i+= 1\n",
    "\n",
    "        df = pd.merge(df,df_ids,on=\"taxid\")\n",
    "        # To remove \"Chordata\",\"Viridiplantae\",\n",
    "        # Will remove at jgi json step \"synthetic\",\"artificial\",\"PREDICTED\"\n",
    "        df_chordata = df[df[\"jgi_tab\"].str.contains(\"Chordata\")==True]\n",
    "        df_virid = df[df[\"jgi_tab\"].str.contains(\"Viridiplantae\")==True]\n",
    "\n",
    "        df_chordata.to_csv(f'{garbage_out}_Chordata.txt',sep=\"\\t\",index=None)\n",
    "        df_virid.to_csv(f'{garbage_out}_Viridiplantae.txt',sep=\"\\t\",index=None)\n",
    "\n",
    "        df_artificial = df[df[\"jgi_tab\"].str.contains(\"synth|vector|Vector|artificial\")==True]\n",
    "        df_artificial.to_csv(f'{garbage_out}_Artificial.txt',sep=\"\\t\",index=None)\n",
    "\n",
    "        df = df[df[\"jgi_tab\"].str.contains(\"Chordata|Viridiplantae\")==False]\n",
    "        df = df[df[\"jgi_tab\"].str.contains(\"synth|vector|Vector|artificial\")==False]\n",
    "\n",
    "        return df        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def toJSON(self,df, fout, name):\n",
    "        import ast\n",
    "        df_J = df[[\"taxid\",\"jgi_json\"]]\n",
    "        del df[\"jgi_json\"]\n",
    "        df.to_json(f\"{fout}_temp.json\",orient='index')\n",
    "        with open(f\"{fout}_temp.json\") as fo:\n",
    "            contig_d = json.load(fo)\n",
    "            for c in contig_d.keys():\n",
    "                d = contig_d[c] \n",
    "                taxid = d[\"taxid\"]  \n",
    "                df_temp = df_J[df_J[\"taxid\"]==taxid]\n",
    "                j = df_temp[\"jgi_json\"].iloc[0]\n",
    "                jdic = ast.literal_eval(j)\n",
    "                d[\"jgi_json\"] = jdic\n",
    "                d[\"filename\"] = name\n",
    "\n",
    "            with open(f\"{fout}.json\", \"w\") as write_file:\n",
    "                json.dump(contig_d, write_file)\n",
    "        removeFile(f\"{fout}_temp.json\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def tophit(self):\n",
    "#         levels =      [\"group_single/single\",\n",
    "#                        \"group_single/group\",\n",
    "#                        \"all_group/group\",\n",
    "#                        \"all_group/all\"]\n",
    "        levels =      [\"all_group/all\"]        \n",
    "        \n",
    "        for level in levels:\n",
    "            print(f\"working on {level}\")\n",
    "\n",
    "            files = glob.glob(f'{self.wkdir}/lastdb_json/{level}/*.json')\n",
    "            for f in files:\n",
    "                name = f'{os.path.basename(f)[:-5]}'\n",
    "                print(f\"working on {name}\")\n",
    "                # Get top hit\n",
    "                with open(f) as fo, open(\"temp.txt\",\"w\") as out1, open(\"nohits.txt\",\"w\") as out2:\n",
    "                    out1.write(\"contig\\ttaxid\\n\")\n",
    "                    out2.write(\"contig\\n\")\n",
    "                    top_d = json.load(fo)[\"BlastOutput2\"]\n",
    "                    for d in top_d:\n",
    "                        r = d[\"report\"][\"results\"][\"search\"]\n",
    "                        contig = r[\"query_title\"]              \n",
    "                        if len(r[\"hits\"]) < 1:\n",
    "                            out2.write(f'{contig}\\n')\n",
    "                        else:\n",
    "                            hits = r[\"hits\"][0]\n",
    "                            if hits[\"num\"] == 1:\n",
    "                                taxid = hits[\"description\"][0][\"taxid\"]\n",
    "                                out1.write(f'{contig}\\t{taxid}\\n')\n",
    "\n",
    "\n",
    "                                \n",
    "                # Write out fasta and match\n",
    "\n",
    "                def writeout(folder,file, nohits):\n",
    "                    df = pd.read_csv(file,sep=\"\\t\")\n",
    "                    makeFolders([f'{self.wkdir}/',\n",
    "                                 f'{self.wkdir}/{folder}/{level}',\n",
    "                                 f'{self.wkdir}/{folder}/{level}/blastXhits'])\n",
    "                              \n",
    "                    fasta = f'{self.wkdir}/lastdb_fasta/{level}/{name}.fasta'\n",
    "                    fout = f'{self.wkdir}/{folder}/{level}/{name}'\n",
    "                    garbage_name = f'{self.wkdir}/{folder}/{level}/blastXhits/{name}'\n",
    "                    \n",
    "                    \n",
    "                    if nohits:\n",
    "                        df = self.matchFasta(df=df, fasta = fasta, fout = fout, nohits=nohits)\n",
    "                        \n",
    "                        \n",
    "                    else:   \n",
    "                        df = self.JGIQuery(df = df, garbage_out = garbage_name)\n",
    "                        df = self.matchFasta(df=df, fasta = fasta, fout = fout, nohits=nohits)\n",
    "                        self.toJSON(df=df, fout=fout, name=name)\n",
    "                        removeFile(file)\n",
    "\n",
    "                writeout(folder=\"FINAL_Nohits\",file=\"nohits.txt\", nohits=True)\n",
    "                writeout(folder=\"FINAL_Tophits\",file=\"temp.txt\", nohits=False)\n",
    "\n",
    "\n",
    "                        \n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.803986Z",
     "start_time": "2020-09-19T21:55:35.801368Z"
    }
   },
   "outputs": [],
   "source": [
    "# NF_out = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/Z_RNAseq-Biome-master/NF_OUT\"\n",
    "# cpus = 4 \n",
    "# F = darkFilterJGI(NF_out=NF_out, cpus=cpus)\n",
    "# F.tophit()\n",
    "# nr_db=\"\"\n",
    "\n",
    "# L = darkLASTDB(NF_out=NF_out, col_data=col_data, cpus=cpus, nr_db=nr_db)\n",
    "# L.darkmakeColDF()\n",
    "# L.darklastIndex()\n",
    "# L.darkruncleanLast() \n",
    "# L.rundarkblastX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:35.972554Z",
     "start_time": "2020-09-19T21:55:35.970778Z"
    }
   },
   "outputs": [],
   "source": [
    "# NF_out = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/nextflow/RNAseq-Biome-Nextflow/NF_OUT\"\n",
    "# cpus = 4 \n",
    "# L = LASTDB(NF_out=NF_out, col_data=col_data, cpus=cpus)\n",
    "# L.makeColDF()\n",
    "# L.runFilterJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:36.676574Z",
     "start_time": "2020-09-19T21:55:36.671461Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import argparse\n",
    "def parse_arguments():\n",
    "        parser = argparse.ArgumentParser(description='filter quantify and graph')\n",
    "        parser.add_argument('--col_data', action= 'store', metavar='col_data') \n",
    "#         parser.add_argument('--f', action= 'store', metavar='f') \n",
    "        parser.add_argument('--Nextflow_Out', action= 'store', metavar='--Nextflow_Out') \n",
    "        parser.add_argument('--cpus', action= 'store', metavar='--cpus')\n",
    "        parser.add_argument('--nr_db', action= 'store', metavar='--nr_db')\n",
    "        args = parser.parse_args()\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:37.105415Z",
     "start_time": "2020-09-19T21:55:37.098093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--col_data col_data]\n",
      "                             [--Nextflow_Out --Nextflow_Out] [--cpus --cpus]\n",
      "                             [--nr_db --nr_db]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/m/Library/Jupyter/runtime/kernel-c2420f48-3c18-45e0-8865-bfe8e51c67b8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "if __name__==\"__main__\":\n",
    "    args = parse_arguments()\n",
    "    cpus = args.cpus\n",
    "    col_data = args.col_data\n",
    "    NF_out = args.Nextflow_Out\n",
    "    nr_db = args.nr_db\n",
    "    #final path /2019/biome/NF_OUT/unmapped/final\n",
    "    \n",
    "    #Run all the commands\n",
    "    L = darkLASTDB(NF_out=NF_out, col_data=col_data, cpus=cpus, nr_db=nr_db)\n",
    "    L.darkmakeColDF()\n",
    "    L.darklastIndex()\n",
    "    L.darkruncleanLast() \n",
    "    L.rundarkblastX()\n",
    "    F = darkFilterJGI(NF_out=NF_out, cpus=cpus)\n",
    "    F.tophit()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T21:55:39.481102Z",
     "start_time": "2020-09-19T21:55:39.198638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1.0-FilterDarkGenome.ipynb to nb_1.0-FilterDarkGenome.py\r\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "!python3 notebook2script.py 1.0-FilterDarkGenome.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
