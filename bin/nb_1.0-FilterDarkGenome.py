
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/1.0-FilterDarkGenome.ipynb
import json
import sys
import os
import os.path
from pathlib import Path
from subprocess import call
import subprocess
import sys # system libraries, like arguments (argv)
import re # regular expressions
import pandas as pd
import glob
import os.path
from pathlib import Path


from timeit import default_timer as timer
import time



def makeFolders(folder_list):
    for directory in folder_list:
        if not directory: # Make sure not an empty string ""
            continue
        else:
            if not os.path.exists(directory):
                os.makedirs(directory)

def removeFile(filename):
    try:
        os.remove(filename)
    except OSError:
        pass

def makeColDF(col_data):
    df = pd.read_csv(col_data,sep="\t",names = ["sample","condition"])
    condition_list = df["condition"].unique()
    return df, condition_list


class darkLASTDB():
    def __init__(self, NF_out, col_data, nr_db, cpus=4,percent_identity=60.0):
        self.NF_out = NF_out
        self.wkdir = f"{self.NF_out}/unmapped/final/darkbiome"
        self.cpus = cpus
        self.col_data = col_data
        self.percent_identity = percent_identity
        self.single = []
        self.group = []
        self.group_FS = []
        self.all = []
        self.grab_list = []
        self.grab_list_name = []
        self.nr_db = nr_db
        self.topHeir = False

    def darkmakeColDF(self):
        self.df_col = pd.read_csv(self.col_data,sep="\t",names = ["sample","condition"])
        self.cl = self.df_col["condition"].unique()

    def darklastIndex(self):
        makeFolders([f'{self.wkdir}/lastdb',f'{self.wkdir}/lastdb/lastdb_index'])

        for c in self.cl:
            index_out = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_{c}'
            makeFolders([index_out])
            fa = f'{self.wkdir}/afterNext/group/group_{c}_darkbiome.fasta'
            self.removeFaDup(fa)
            index_cmd = f'lastdb -cR01 {index_out}/group_{c} {fa}'
            subprocess.run(index_cmd, shell=True)
            self.group.append(fa)

        index_out = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_all'
        makeFolders([index_out])
        fa = f'{self.wkdir}/afterNext/all/all_darkbiome.fasta'
        index_cmd = f'lastdb -cR01 {index_out}/all {fa}'
        subprocess.run(index_cmd, shell=True)
        self.all.append(fa)


    def darkrunLast(self):
        f_in = f'{self.wkdir}/afterNext/single/'
        out = f'{self.wkdir}/lastdb/lastdb_out/group_single'
        makeFolders([f'{self.wkdir}/lastdb/lastdb_out/',out])
        self.single = []

        if self.topHeir == False:
            for x in range(len(self.df_col)):
                c = self.df_col["condition"].iloc[x]
                name = self.df_col["sample"].iloc[x]
                fa = f_in+name+"_darkbiome.fasta"
                self.removeFaDup(fa)
                index = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_{c}/group_{c}'
                last_cmd = f'lastal -f BlastTab+ {index} {fa} > {out}/{name}.tab'
                subprocess.run(last_cmd, shell=True)
                self.single.append(fa)


        if self.topHeir == True:
            #Map condition to all
            out = f'{self.wkdir}/lastdb/lastdb_out/all_group'
            makeFolders([out])
            for c in self.cl:
                fa = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/group/group_{c}_darkbiome.fasta'
                name = f'group_{c}'
                index = f'{self.wkdir}/lastdb/lastdb_index/lastdb_index_all/all'
                last_cmd = f'lastal -f BlastTab+ {index} {fa} > {out}/{name}.tab'
                subprocess.run(last_cmd, shell=True)
                self.group_FS.append(fa)



    def darkcleanLast(self, tab_path, query, subject):
        #query is lower on totem pole
        df = pd.DataFrame()
        for f in glob.glob(tab_path+"/*.tab"):
            print(f)
            df_t = pd.read_csv(f, sep="\t", skiprows=26,
                              names=["query_id","subject_id","percent_identity",
                                     "alignment_length","mismatches","gap_opens",
                                     "q_start","q_end","s_start","s_end","evalue",
                                     "bit_score","query_length","subject_length",
                                     "raw_score"])
            df_t=df_t[~df_t["query_id"].str.contains("#")]
            print(df)
            df = pd.concat([df,df_t])

        print(df)
        df = df[df["percent_identity"]>self.percent_identity]
        q_df = df[["query_id"]]
        s_df = df[["subject_id"]]
        q_df = q_df.rename(columns={"query_id":"contig"})
        s_df = s_df.rename(columns={"subject_id":"contig"})
        q_df = q_df.drop_duplicates(subset="contig")
        s_df = s_df.drop_duplicates(subset="contig")

        out = f'{self.wkdir}/lastdb/lastdb_cleaned/{query}_{subject}'
        makeFolders([f'{self.wkdir}/lastdb/lastdb_cleaned/',out])
        q_df.to_csv(f'{out}/{query}_filteredBy_{subject}.txt',sep="\t", index=None, header=None)
        s_df.to_csv(f'{out}/{subject}_filteredBy_{query}.txt',sep="\t", index=None, header=None)
        df_grab = pd.concat([q_df,s_df])
        df_grab.to_csv(f'{out}/grab_{subject}_{query}.txt',sep="\t",
                           index=None, header=None)
        self.grab_list += [df_grab]
        self.grab_list_name += [f'{out}/grab_{subject}_{query}.txt']


    def darkfilterCleaned(self, out, fasta_list, grab):
        for f in fasta_list:
            name = os.path.basename(f)
            cmd  = f'seqtk subseq {f} {grab} > {out}/{name}'
            subprocess.run(cmd, shell=True)

    def darkruncleanLast(self):
        for data in ["json","df","fasta"]:
            makeFolders([f'{self.wkdir}/lastdb/lastdb_{data}/',
                     f'{self.wkdir}/lastdb/lastdb_{data}/group_single',
                     f'{self.wkdir}/lastdb/lastdb_{data}/group_single/single',
                     f'{self.wkdir}/lastdb/lastdb_{data}/group_single/group',
                     f'{self.wkdir}/lastdb/lastdb_{data}/all_group',
                     f'{self.wkdir}/lastdb/lastdb_{data}/all_group/all',
                     f'{self.wkdir}/lastdb/lastdb_{data}/all_group/group',
                    ])


        #Clean singles and groups
        self.darkrunLast()
        self.darkcleanLast(tab_path=f'{self.wkdir}/lastdb/lastdb_out/group_single',
                      query="single", subject="group")
        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/single',
                      fasta_list = self.single,
                      grab = self.grab_list_name[0])
        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/group',
                      fasta_list = self.group,
                      grab = self.grab_list_name[0])



        self.topHeir = True # Change this to true so you can start function below
        self.darkrunLast()
        #Clean group filtered by single and all
        self.darkcleanLast(tab_path=f'{self.wkdir}/lastdb/lastdb_out/all_group',
                       query="group", subject="all")
        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/group',
                      fasta_list = self.group_FS,
                      grab = self.grab_list_name[1])
        self.darkfilterCleaned(out = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/all',
                      fasta_list = self.all,
                      grab = self.grab_list_name[1])


    def removeFaDup(self, f1):
        tmp = f1+"tmp"
        with open(f1, "r") as infile, open(tmp,"w") as outfile:
            namelist = []
            WRITE = True
            for line in infile:
                if ">" in line:
                    name = line
                    if name in namelist:
                        WRITE = False
                    else:
                        WRITE = True
                    namelist.append(name)
                if WRITE:
                    outfile.write(line)

        with open(tmp,"r") as infile, open(f1,"w") as outfile:
            for line in infile:
                outfile.write(line)
        removeFile(tmp)











    def darkblastX(self, path,out):
        for fa in glob.glob(path+"/*.fasta"):
            l = len(".fasta")
            name = f'{os.path.basename(fa)[:-l]}'
            self.removeFaDup(fa)
            cmd  = f'blastx -db {self.nr_db} -query {fa} -max_target_seqs 10 -outfmt 15 -num_threads {cpus} -out {out}/{name}.json'
            subprocess.run(cmd, shell=True)

    def rundarkblastX(self):
#         self.darkblastX(path = f'{self.wkdir}/lastdb/lastdb_fasta/group_single/single',
#                 out = f'{self.wkdir}/lastdb/lastdb_json/group_single/single')
#         self.darkblastX(path = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/group',
#                 out = f'{self.wkdir}/lastdb/lastdb_json/all_group/group')
        self.darkblastX(path = f'{self.wkdir}/lastdb/lastdb_fasta/all_group/all',
                out = f'{self.wkdir}/lastdb/lastdb_json/all_group/all')



class darkFilterJGI():
    def __init__(self, NF_out, cpus=4):
        self.NF_out = NF_out
        self.wkdir = f'{self.NF_out}/unmapped/final/darkbiome/lastdb'
        self.cpus = cpus


    def matchFasta(self, df, fasta, fout, nohits):
        c11 = fout+"_newfasta1.fast"
        c12 = fout+"_newfasta2.fast"
        with open(fasta,'r', newline="\n") as infile, open(c11, 'w') as outfile:
            for line in infile:
                if ">" in line:
                    line = line.replace('\n', '\t')
                    line = line.replace('>', '\n')
                    outfile.write(line)
                else:
                    line = line.replace('\n', 'RTN666')
                    outfile.write(line)
            infile.close()
            outfile.close()

        with open(c11,'r', newline="\n") as infile, open(c12, 'w') as outfile:
            for line in infile:
                if "NODE" not in line:
                    line = line.replace('\n', '')
                    line = line.replace('\r', '')
                    outfile.write(line)
                else:
                    outfile.write(line)
            infile.close()
            outfile.close()

        my_cols=['contig', 'fasta_with_returnsAdded']
        df_fa=pd.read_csv(c12, index_col=['contig'], sep='\t', names=my_cols)
        df = pd.merge(df, df_fa, on='contig') # Dont run this cell more than once
        df["fasta"] = df['fasta_with_returnsAdded'].str.replace("RTN666","")
        df["length"] = df["fasta"].str.len()

        # Write out fasta
        with open(fout+".fasta","w") as outfile:
            for x in range(len(df)):
                nodename = ">"+df["contig"].iloc[x]+"\n"
                fa = df['fasta_with_returnsAdded'].iloc[x]
                fa = fa.replace('RTN666', '\n')
                outfile.write(nodename+fa)

        if nohits:
            df = df[["contig","length","fasta"]]
        else:
            df = df[["contig","taxid","length","jgi_tab","jgi_json",
                    "fasta"]]
        df.to_csv(fout+"_df.txt",sep="\t",index=None)
        removeFile(c11)
        removeFile(c12)
        return df


    def JGIQuery(self, df, garbage_out):
        garbage_out = garbage_out[:-4]
        df = df.copy()
        taxids=df['taxid'].unique()
        df_ids = pd.DataFrame(taxids)
        df_ids = df_ids.rename(columns = {0:"taxid"})

        df_ids["jgi_tab"] = "NA"
        df_ids["jgi_json"] = "NA"
        i = 0
        R = len(df_ids)
        for x in range(R):
            print(f'Querying {i+1} of {R} taxids')
            id = df_ids["taxid"].iloc[x]
            Q1 = "curl https://taxonomy.jgi-psf.org/sc/id/" + str(id)
            result = os.popen(Q1).read()
            df_ids["jgi_tab"].iloc[x] = result

            #Get json return
            jasonQ1 = "curl https://taxonomy.jgi-psf.org/id/" + str(id)
            result = os.popen(jasonQ1).read()
            j = json.loads(result)
            j = j[list(j.keys())[0]]
            j = str(j)
            df_ids["jgi_json"].iloc[x] = j

            i+= 1

        df = pd.merge(df,df_ids,on="taxid")
        # To remove "Chordata","Viridiplantae",
        # Will remove at jgi json step "synthetic","artificial","PREDICTED"
        df_chordata = df[df["jgi_tab"].str.contains("Chordata")==True]
        df_virid = df[df["jgi_tab"].str.contains("Viridiplantae")==True]

        df_chordata.to_csv(f'{garbage_out}_Chordata.txt',sep="\t",index=None)
        df_virid.to_csv(f'{garbage_out}_Viridiplantae.txt',sep="\t",index=None)

        df_artificial = df[df["jgi_tab"].str.contains("synth|vector|Vector|artificial")==True]
        df_artificial.to_csv(f'{garbage_out}_Artificial.txt',sep="\t",index=None)

        df = df[df["jgi_tab"].str.contains("Chordata|Viridiplantae")==False]
        df = df[df["jgi_tab"].str.contains("synth|vector|Vector|artificial")==False]

        return df




    def toJSON(self,df, fout, name):
        import ast
        df_J = df[["taxid","jgi_json"]]
        del df["jgi_json"]
        df.to_json(f"{fout}_temp.json",orient='index')
        with open(f"{fout}_temp.json") as fo:
            contig_d = json.load(fo)
            for c in contig_d.keys():
                d = contig_d[c]
                taxid = d["taxid"]
                df_temp = df_J[df_J["taxid"]==taxid]
                j = df_temp["jgi_json"].iloc[0]
                jdic = ast.literal_eval(j)
                d["jgi_json"] = jdic
                d["filename"] = name

            with open(f"{fout}.json", "w") as write_file:
                json.dump(contig_d, write_file)
        removeFile(f"{fout}_temp.json")




    def tophit(self):
#         levels =      ["group_single/single",
#                        "group_single/group",
#                        "all_group/group",
#                        "all_group/all"]
        levels =      ["all_group/all"]

        for level in levels:
            print(f"working on {level}")

            files = glob.glob(f'{self.wkdir}/lastdb_json/{level}/*.json')
            for f in files:
                name = f'{os.path.basename(f)[:-5]}'
                print(f"working on {name}")
                # Get top hit
                with open(f) as fo, open("temp.txt","w") as out1, open("nohits.txt","w") as out2:
                    out1.write("contig\ttaxid\n")
                    out2.write("contig\n")
                    top_d = json.load(fo)["BlastOutput2"]
                    for d in top_d:
                        r = d["report"]["results"]["search"]
                        contig = r["query_title"]
                        if len(r["hits"]) < 1:
                            out2.write(f'{contig}\n')
                        else:
                            hits = r["hits"][0]
                            if hits["num"] == 1:
                                taxid = hits["description"][0]["taxid"]
                                out1.write(f'{contig}\t{taxid}\n')



                # Write out fasta and match

                def writeout(folder,file, nohits):
                    df = pd.read_csv(file,sep="\t")
                    makeFolders([f'{self.wkdir}/',
                                 f'{self.wkdir}/{folder}/{level}',
                                 f'{self.wkdir}/{folder}/{level}/blastXhits'])

                    fasta = f'{self.wkdir}/lastdb_fasta/{level}/{name}.fasta'
                    fout = f'{self.wkdir}/{folder}/{level}/{name}'
                    garbage_name = f'{self.wkdir}/{folder}/{level}/blastXhits/{name}'


                    if nohits:
                        df = self.matchFasta(df=df, fasta = fasta, fout = fout, nohits=nohits)


                    else:
                        df = self.JGIQuery(df = df, garbage_out = garbage_name)
                        df = self.matchFasta(df=df, fasta = fasta, fout = fout, nohits=nohits)
                        self.toJSON(df=df, fout=fout, name=name)
                        removeFile(file)

                writeout(folder="FINAL_Nohits",file="nohits.txt", nohits=True)
                writeout(folder="FINAL_Tophits",file="temp.txt", nohits=False)






import argparse
def parse_arguments():
        parser = argparse.ArgumentParser(description='filter quantify and graph')
        parser.add_argument('--col_data', action= 'store', metavar='col_data')
#         parser.add_argument('--f', action= 'store', metavar='f')
        parser.add_argument('--Nextflow_Out', action= 'store', metavar='--Nextflow_Out')
        parser.add_argument('--cpus', action= 'store', metavar='--cpus')
        parser.add_argument('--nr_db', action= 'store', metavar='--nr_db')
        args = parser.parse_args()
        return args


if __name__=="__main__":
    args = parse_arguments()
    cpus = args.cpus
    col_data = args.col_data
    NF_out = args.Nextflow_Out
    nr_db = args.nr_db
    #final path /2019/biome/NF_OUT/unmapped/final

    #Run all the commands
    L = darkLASTDB(NF_out=NF_out, col_data=col_data, cpus=cpus, nr_db=nr_db)
    L.darkmakeColDF()
    L.darklastIndex()
    L.darkruncleanLast()
    L.rundarkblastX()
    F = darkFilterJGI(NF_out=NF_out, cpus=cpus)
    F.tophit()

