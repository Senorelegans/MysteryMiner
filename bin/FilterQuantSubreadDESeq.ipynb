{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:45:47.777614Z",
     "start_time": "2020-05-06T17:45:47.773491Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from subprocess import call\n",
    "import subprocess\n",
    "import sys # system libraries, like arguments (argv)\n",
    "import re # regular expressions\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "# from nb_FilterLastDB import *\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:45:48.143877Z",
     "start_time": "2020-05-06T17:45:48.140524Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def makeFolders(folder_list):\n",
    "    for directory in folder_list:\n",
    "        if not directory: # Make sure not an empty string \"\"\n",
    "            continue\n",
    "        else:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter class (Filter contigs of interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:45:49.948737Z",
     "start_time": "2020-05-06T17:45:49.926607Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class JSON():\n",
    "    def __init__(self, f, explicit, min_length = 100):\n",
    "        self.f = f\n",
    "        self.min_length = min_length\n",
    "        self.json_empty = False\n",
    "        self.num_contigs = 0\n",
    "        self.explicit = explicit\n",
    "        with open(f) as fo:\n",
    "            cd = json.load(fo) # cd is contig dictionary\n",
    "            k = list(cd.keys())\n",
    "            self.pe(f'Search starting with {len(k)} contigs')\n",
    "            if len(k) > 0:\n",
    "                self.filename = cd[k[0]][\"filename\"]\n",
    "                self.cd = cd\n",
    "            else:\n",
    "                self.json_empty = True\n",
    "                self.cd = {}\n",
    "                self.pe(f'*-**-**-**-**-**-**-**-**-**-* WARNING file Empty!!!')\n",
    "            \n",
    "    def pe(self, s):\n",
    "        if self.explicit:\n",
    "            print(s)\n",
    "            \n",
    "            \n",
    "    def updateCD(self):\n",
    "        self.k = list(self.cd.keys())\n",
    "        self.num_contigs = len(self.k)\n",
    "        if self.num_contigs < 1:\n",
    "            self.json_empty = True\n",
    "    \n",
    "    def minLength(self):\n",
    "        if self.json_empty == False:\n",
    "            cd = self.cd.copy()\n",
    "            for c in list(self.cd):\n",
    "                length = int(self.cd[c][\"length\"])\n",
    "                if length<self.min_length:\n",
    "                    self.cd.pop(c) \n",
    "            self.updateCD()\n",
    "            self.pe(f'{len(self.k)} contigs after filtering for min length {min_length} (Default 100)')\n",
    "            if J.json_empty: self.pe(f'WARNING!!! Found no contigs after filtering for min length')\n",
    "\n",
    "        \n",
    "               \n",
    "    def searchRank(self, rank, HAS=True):\n",
    "        if self.json_empty == False:\n",
    "            cd_rank = {}\n",
    "            cd_norank = {}\n",
    "            for c in list(self.cd):\n",
    "                contig = self.cd[c]\n",
    "                try:\n",
    "                    rd = self.cd[c][\"jgi_json\"][rank]\n",
    "                    cd_rank[c] = contig\n",
    "                except:\n",
    "                    cd_norank[c] = contig\n",
    "            if HAS:\n",
    "                self.cd = cd_rank\n",
    "            else:\n",
    "                self.cd = cd_norank\n",
    "            self.updateCD()\n",
    "        else:\n",
    "            self.pe(\"No input into rank search\")\n",
    "            self.updateCD()\n",
    "            \n",
    "    \n",
    "    def searchRankName(self, rank, name, HAS=True):\n",
    "        if self.json_empty == False:\n",
    "            self.searchRank(rank=rank, NOT=False)\n",
    "            if self.json_empty == False:\n",
    "                for c in list(self.cd):\n",
    "                    r_d = self.cd[c][\"jgi_json\"][rank]\n",
    "                    if HAS:\n",
    "                        if r_d[\"name\"]!=name:\n",
    "                            self.cd.pop(c)   \n",
    "                    else:\n",
    "                        if r_d[\"name\"]==name:\n",
    "                            self.cd.pop(c)                      \n",
    "                self.updateCD()\n",
    "            else:\n",
    "                self.pe(f\"Nothing in rank {rank}\")\n",
    "                self.updateCD()\n",
    "        else: \n",
    "            self.pe(\"No input into searchRankName\")\n",
    "            self.updateCD()\n",
    "            \n",
    "            \n",
    "    \n",
    "    def searchJ(self,rank=\"NA\", name=\"NA\", HAS=True, FILTER=True):\n",
    "        if FILTER:\n",
    "            if self.json_empty: self.pe(f'json empty')\n",
    "    #         print(f'Removing contigs less than {self.min_length}')\n",
    "            if rank != \"NA\" and name != \"NA\":  \n",
    "                self.searchRankName(rank=rank, name=name, HAS=HAS)\n",
    "                self.pe(f\"{self.num_contigs}:contigs after search for...\\ncontains({HAS}) rank:{rank}\\n  name:{name}\")        \n",
    "            if rank != \"NA\" and name == \"NA\":  \n",
    "                self.searchRank(rank=rank, HAS=HAS)  \n",
    "                self.pe(f\"{self.num_contigs}:contigs after search for...\\ncontains({HAS})  rank:{rank}\")\n",
    "            print(f'{len(self.k)} contigs after filtering')\n",
    "        self.updateCD()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:45:50.196849Z",
     "start_time": "2020-05-06T17:45:50.189377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search starting with 3 contigs\n",
      "1:contigs after search for...\n",
      "contains(False)  rank:phylum\n",
      "1 contigs after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2': {'contig': 'NODE_26_length_223_cov_282.690476_g25_i0',\n",
       "  'blast_pident': 'gi|1548994288|gb|CP034492.1|',\n",
       "  'blast_sseqid': '100.000',\n",
       "  'blast_evalue': '4.98e-83',\n",
       "  'taxid': '111789',\n",
       "  'blast_stitle': 'Eukaryotic synthetic construct chromosome 14',\n",
       "  'filename': 'test',\n",
       "  'fasta_with_returnsAdded': 'GCATACGCGATCAGTTCCGGTGACTGGAGTTCGACGTGTGCTCTTCCGATCCGGTGGCGCRTN666GTGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCTGGAGGATCGCTTGAGTCCAGGAGTRTN666TCTGGGCTGTAGTGCGCTATGCCGATCGGGTGTCCGCACTAAGTTCGGCATCAATATGGTRTN666GACCTCCCGGGAGCGGGGGACCACCAGGTTGCCTAAGGAGGGGRTN666',\n",
       "  'fasta': 'GCATACGCGATCAGTTCCGGTGACTGGAGTTCGACGTGTGCTCTTCCGATCCGGTGGCGCGTGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCTGGAGGATCGCTTGAGTCCAGGAGTTCTGGGCTGTAGTGCGCTATGCCGATCGGGTGTCCGCACTAAGTTCGGCATCAATATGGTGACCTCCCGGGAGCGGGGGACCACCAGGTTGCCTAAGGAGGGG',\n",
       "  'length': 223,\n",
       "  'jgi_json': {'name': 'eukaryotic synthetic construct',\n",
       "   'tax_id': 111789,\n",
       "   'level': 'species',\n",
       "   'species': {'name': 'eukaryotic synthetic construct', 'tax_id': 111789},\n",
       "   'no rank': {'name': 'eukaryotic synthetic constructs', 'tax_id': 111786},\n",
       "   'no rank 2': {'name': 'artificial sequences', 'tax_id': 81077},\n",
       "   'no rank 3': {'name': 'other sequences', 'tax_id': 28384}}}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = \"kingdom\"\n",
    "label = \"Metazoa\"\n",
    "min_length = 100\n",
    "\n",
    "p = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/nextflow/RNAseq-Biome-Nextflow/bin/test/jgi_test/\"\n",
    "f1=p+\"test_jgi.json\"    \n",
    "\n",
    "# J = makeJ(f, min_length, NOT=False)\n",
    "# J = makeJ(f, min_length, NOT=False, rank=\"phylum\")\n",
    "# self.cd = self.searchRankName(self.cd,'kingdom','Metazoa')\n",
    "# J = makeJ(f, min_length, NOT=True, rank=\"kingdom\", name = 'Metazoa')\n",
    "\n",
    "\n",
    "J = JSON(f=f1, min_length=min_length, explicit=True)\n",
    "J.searchJ(HAS=False, rank=\"phylum\")\n",
    "# J = searchJ(J, min_length, NOT=True, rank=\"species\", name = 'eukaryotic synthetic construct')\n",
    "# J = searchJ(J, min_length, NOT=True, rank=\"phylum\", name = 'Platyhelminthes')\n",
    "# J = searchJ(J, min_length, NOT=True, rank=\"phylum\", name  = 'Platyhelminthes')\n",
    "# J = searchJ(J, min_length, NOT=False, rank=\"phylum\")\n",
    "# J = searchJ(J, min_length, NOT=False, rank=\"species\", name = 'eukaryotic synthetic construct')\n",
    "# J = searchJ(J, min_length, NOT=False, rank=\"species\", name = 'eukaryotic synthetic construct')\n",
    "\n",
    "\n",
    "# Not will remove everything not in the Rank. \n",
    "# If you include name it will return the rank and remove everything that doesnt have that name\n",
    "\n",
    "\n",
    "J.num_contigs\n",
    "J.cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Jsons after query and make gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:45:50.570119Z",
     "start_time": "2020-05-06T17:45:50.523799Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Quantify():\n",
    "    \n",
    "    def __init__(self, NF_out, title, searchlist, \n",
    "                 NF_path, col_data, cpus, \n",
    "                 singleEnd,  explicit, out_path,\n",
    "                 control, treatment, binContigs,  min_length,\n",
    "                 binRank=\"blank\"):\n",
    "        self.binContigs = binContigs\n",
    "        self.binRank = binRank\n",
    "        self.control = control\n",
    "        self.treatment = treatment\n",
    "        self.explicit = explicit\n",
    "        self.cpus = cpus\n",
    "        self.singleEnd = singleEnd\n",
    "        self.NF_out = NF_out\n",
    "        self.NF_path = NF_path\n",
    "        self.wkdir = f'{self.NF_out}/unmapped/final'\n",
    "        self.title = title\n",
    "        self.Jlist_d = {}\n",
    "        self.df_norm = pd.DataFrame()\n",
    "        self.out_path = out_path\n",
    "        self.searchlist = searchlist\n",
    "        self.col_data = col_data\n",
    "        self.min_length = min_length\n",
    "        self.df_col = pd.read_csv(self.col_data,sep=\"\\t\",names = [\"sample\",\"condition\"])\n",
    "        self.condition_list = self.df_col[\"condition\"].unique()\n",
    "#         self.levels = [\"single\",\"group\",\"all\",\n",
    "#                        \"dark_single_FG\",\"dark_group_FA\",\"dark_all_FG\"]\n",
    "        self.levels = [\"all\",\"dark_all_FG\"]\n",
    "\n",
    "    def pe(self, s):\n",
    "        if self.explicit:\n",
    "            print(s)        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    def toJSON(self):\n",
    "        import ast\n",
    "        \n",
    "        for f in glob.glob(f'{self.wkdir}jgi_df/*/*'):\n",
    "#         for f in glob.glob(f'{self.wkdir}+jgi_df/*/*')\n",
    "            l = len(\"_df.txt\")\n",
    "            fout = f[:-l]\n",
    "        \n",
    "            df_J = df[[\"taxid\",\"jgi_json\"]]\n",
    "            del df[\"jgi_json\"]\n",
    "            df.to_json(f\"{fout}_temp.json\",orient='index')\n",
    "            with open(f\"{fout}_temp.json\") as fo:\n",
    "                contig_d = json.load(fo)\n",
    "                for c in contig_d.keys():\n",
    "                    d = contig_d[c] \n",
    "                    taxid = d[\"taxid\"]  \n",
    "                    df_temp = df_J[df_J[\"taxid\"]==taxid]\n",
    "                    j = df_temp[\"jgi_json\"].iloc[0]\n",
    "                    jdic = ast.literal_eval(j)\n",
    "                    d[\"jgi_json\"] = jdic\n",
    "                    d[\"filename\"] = name\n",
    "\n",
    "                with open(f\"{fout}.json\", \"w\") as write_file:\n",
    "                    json.dump(contig_d, write_file)\n",
    "            removeFile(f\"{fout}_temp.json\")\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def searchContigs(self):\n",
    "        for level in self.levels:\n",
    "            Jlist = []\n",
    "            if self.explicit:\n",
    "                print(f'Working on {level} --- ')\n",
    "            if level == \"single\" or \"group\" or \"all\":\n",
    "                jsons = glob.glob(f'{self.wkdir}/jgi_df/{level}/*.json')\n",
    "            \n",
    "            #Dark genome\n",
    "            folder = \"darkbiome/lastdb/FINAL_Tophits\"\n",
    "            \n",
    "#             \n",
    "#             /Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/Z_RNAseq-Biome-master/NF_OUT/unmapped/final/darkbiome/lastdb/FINAL_Tophits/all_group/all/all_darkbiome_df.txt\n",
    "            if level == \"dark_single_FG\":\n",
    "                jsons = glob.glob(f'{self.wkdir}/{folder}/group_single/single/*.json')               \n",
    "            if level == \"dark_group_FA\":\n",
    "                jsons = glob.glob(f'{self.wkdir}/{folder}/all_group/group/*.json')                   \n",
    "            if level == \"dark_all_FG\":\n",
    "                jsons = glob.glob(f'{self.wkdir}/{folder}/all_group/all/*.json')    \n",
    "            \n",
    "            \n",
    "#             print(f'{self.wkdir}/{folder}/all_group/all/')\n",
    "#             print(jsons)\n",
    "                \n",
    "            for f in jsons:\n",
    "                print(\"---------------\")\n",
    "                print(f' Working on file {f}')\n",
    "                J = JSON(f=f, explicit=self.explicit)\n",
    "#                 J.minLength()\n",
    "\n",
    "                if J.json_empty == False:\n",
    "                    Jlist.append(J)  \n",
    "            \n",
    "            if len(Jlist) < 1:\n",
    "                print(\"*_*_*_*_*_*_*_ WARNING *_*_*_*_*_*_*_\")\n",
    "                print(f\"No files in {level} had any contigs\")\n",
    "            else:\n",
    "                self.Jlist_d[level] = Jlist\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    def makeGTF_and_df(self):\n",
    "        makeFolders([f'{self.out_path}/{self.title}'])\n",
    "        for level in self.levels:\n",
    "            out = f'{self.out_path}/{self.title}/{level}'\n",
    "            makeFolders([out])\n",
    "            f_gtf = f'{out}/{level}_{self.title}.gtf'\n",
    "            f_df = f'{out}/{level}_{self.title}.tsv'\n",
    "            \n",
    "            open(f_gtf,\"w\")\n",
    "            df_final = pd.DataFrame()\n",
    "            with open(f_gtf,\"a\") as outfile:\n",
    "                for J in self.Jlist_d[level]:\n",
    "                    for c in J.cd:\n",
    "                        contig_d = J.cd[c]\n",
    "                        contig_name = contig_d[\"contig\"]\n",
    "                        length = contig_d[\"length\"]\n",
    "                        if self.binRank != \"NA\":\n",
    "                            binrank = contig_d['jgi_json'][self.binRank][\"name\"]\n",
    "                        else:\n",
    "                            binrank = contig_name\n",
    "                        line = f'{contig_name}\\tblank\\texon\\t1\\t{length}\\t.\\t+\\t.\\t{self.binRank} \"{binrank}\"\\n'\n",
    "                        outfile.write(line)\n",
    "                    df = pd.DataFrame.from_dict(J.cd).T\n",
    "                    df = df[[\"filename\",\"contig\",\"length\",\"taxid\",\"jgi_tab\",\"fasta\"]]\n",
    "                    df_final = pd.concat([df_final,df])\n",
    "            df_final.to_csv(f_df,sep=\"\\t\",index=None)\n",
    "            if len(df_final) > 0:\n",
    "                self.runR(out = out, level=level, gtf = f_gtf)\n",
    "            else:\n",
    "                with open(f'{self.out_path}/{self.title}/Empty_files.txt',\"w\") as outfile:\n",
    "                    outfile.write(os.path.basename(f_gtf)+\"\\n\")\n",
    "            \n",
    "            \n",
    "    def get_star_output(self):\n",
    "        \"\"\"This function will return a df from star of all reads\"\"\"\n",
    "        starpath = f'{self.NF_out}/qc/star_mapstats'\n",
    "        filelist = glob.glob(starpath+\"/*Log.final.out\")\n",
    "        df = pd.DataFrame(columns=[\"sample\",\"input\",\"unique\",\"multi\"])\n",
    "        x = 0\n",
    "        \n",
    "        \n",
    "        for f1 in filelist:\n",
    "            with open(f1,\"r\") as infile:\n",
    "                input_reads_list = []\n",
    "                unique_reads_list = []\n",
    "                multi_reads_list = []\n",
    "                for line in infile:\n",
    "                    if \"Number of input reads\" in line:\n",
    "                        scl = [m.start() for m in re.finditer(r\"\\t\",line)]  \n",
    "                        input_reads = line[ scl[0]:].strip(\"\\t\")\n",
    "                        input_reads_list.append(int(input_reads))\n",
    "                    if \"Uniquely mapped reads number\" in line:\n",
    "                        scl = [m.start() for m in re.finditer(r\"\\t\",line)]  \n",
    "                        unique_reads = line[ scl[0]:].strip(\"\\t\")\n",
    "                        unique_reads_list.append(int(unique_reads))\n",
    "                    if \"Number of reads mapped to multiple loci\" in line:\n",
    "                        scl = [m.start() for m in re.finditer(r\"\\t\",line)]  \n",
    "                        multi_reads = line[ scl[0]:].strip(\"\\t\")\n",
    "                        multi_reads_list.append(int(multi_reads))\n",
    "                sample = os.path.basename(f1).replace(\"Log.final.out\",\"\")\n",
    "                df.loc[x] = pd.Series({\"sample\":sample, \"unique\": unique_reads_list[0], \"input\":input_reads_list[0],\"multi\":multi_reads_list[0]})\n",
    "            x = x + 1\n",
    "\n",
    "        df = get_star_output()\n",
    "        df[\"total_mapped\"] = (df[\"unique\"] + df[\"multi\"]).astype(int)\n",
    "        df[\"unmapped\"] = df[\"input\"] - df[\"total_mapped\"]\n",
    "\n",
    "        df[\"total_mapped_avg\"] = df[\"total_mapped\"].mean()\n",
    "        df[\"total_mapped/total_mapped_avg\"] = df[\"total_mapped\"] / df[\"total_mapped_avg\"]            \n",
    "        df.to_csv(starpath+\"/starmapstats.txt\",sep=\"\\t\",index=None)\n",
    "        self.df_norm = df\n",
    "        \n",
    "            \n",
    "            \n",
    "    def runR(self, out, level, gtf):\n",
    "        if self.singleEnd:\n",
    "            PE = \"FALSE\"\n",
    "        else:\n",
    "            PE = \"TRUE\"\n",
    "            \n",
    "        if self.binContigs:\n",
    "            MF = \"TRUE\"\n",
    "        else:\n",
    "            MF = \"FALSE\"                  \n",
    "    \n",
    "        #Make col data file\n",
    "        if level == \"single\" or level ==\"dark_single_FG\":\n",
    "            bamfront = f'{self.wkdir}/bam/single/'\n",
    "            bamend = \"_unmapped_single.bam\"\n",
    "\n",
    "        if level == \"group\" or level ==\"dark_group_FA\":\n",
    "            bamfront = f'{self.wkdir}/bam/group/'\n",
    "            bamend = \"_unmapped_group.bam\"\n",
    "            \n",
    "        if level == \"all\" or level ==\"dark_all_FG\":\n",
    "            bamfront = f'{self.wkdir}/bam/all/'\n",
    "            \n",
    "#             Biome-master/NF_OUT/unmapped/final/bam/all\n",
    "            bamend = \"_unmapped_all.bam\"            \n",
    "            \n",
    "        df = self.df_col.copy()\n",
    "        \n",
    "        # Run on all samples\n",
    "#         df = df.loc[df['condition'].isin([self.treatment,self.control])]\n",
    "#         print(df)\n",
    "        df[\"sample2\"] = bamfront + df[\"sample\"] + bamend\n",
    "        #Make bamlist for dseq\n",
    "        bamlist = df[\"sample2\"].tolist()\n",
    "        bamlist = str(bamlist).strip(\"[\").strip(\"]\")\n",
    "    \n",
    "        # fix col data to match rsubread out\n",
    "#         X513.S37.L002.unmapped.all.bam\n",
    "        df[\"sample\"] = \"X\"+df[\"sample\"].str.replace(\"_\",\".\")+bamend.replace(\"_\",\".\")\n",
    "        del df[\"sample2\"]\n",
    "        df.to_csv(f'{out}/col_data.txt',sep=\"\\t\",index=None, header=None)\n",
    "    \n",
    "    \n",
    "            \n",
    "        #Make template subread and r script\n",
    "        template = f'{self.NF_path}/bin/R_subread_DEseq2_TEMPLATE.R'\n",
    "        rscript = f'{out}/{level}.R'\n",
    "        with open(template, \"r\") as infile, open(rscript, \"w\") as outfile:\n",
    "            for line in infile:\n",
    "                line = line.replace(\"bamlist\", bamlist)\n",
    "                line = line.replace(\"outputprefix\", out)\n",
    "                line = line.replace(\"COL_DATA\", f'{out}/col_data.txt')\n",
    "                line = line.replace(\"PAIRED\", PE)\n",
    "                line = line.replace(\"BLANK\", self.binRank)\n",
    "                line = line.replace(\"METAFEATURE\", MF)\n",
    "                line = line.replace(\"CONTROL\", self.control)\n",
    "                line = line.replace(\"TREATMENT\", self.treatment)\n",
    "                line = line.replace(\"temp_annotation\", str(gtf))\n",
    "                line = line.replace(\"cpus\", str(self.cpus))\n",
    "                outfile.write(line)\n",
    "                #R CMD BATCH /D_nonsig.R ${COMPARISON_PATHsig.R.out\n",
    "        cmd = f'R CMD BATCH {rscript} {rscript}.R.out'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "        print(\"done with r\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:46:06.055887Z",
     "start_time": "2020-05-06T17:46:06.051498Z"
    }
   },
   "outputs": [],
   "source": [
    "NF = \"/Users/m/Google_Drive/Scripts/2019/biome/biome_shared/general/datasets/chlamydia/RNAseq-Biome-master\"\n",
    "NF_out = f\"{NF}/NF_OUT\" \n",
    "NF_path = NF\n",
    "out_path = NF\n",
    "col_data = f\"{NF}/sample_table.txt\"\n",
    "singleEnd = False\n",
    "control = \"one_POLYA\"\n",
    "treatment = \"TW4_POLYA\"\n",
    "# searchlist = [(True,\"phylum\",\"NA\")]\n",
    "searchlist = None\n",
    "binRank = \"NA\"\n",
    "binContigs = False\n",
    "min_length = 100\n",
    "# searchlist = [(True,\"phylum\",\"NA\"), (False,\"phylum\",\"Bacteria_name\")]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q = Quantify(NF_out=NF_out, \n",
    "#              NF_path=NF_path,\n",
    "#              out_path = out_path, \n",
    "#              col_data = col_data, \n",
    "#              cpus=4, \n",
    "#              singleEnd=False, \n",
    "#              title=\"HasPhylum\", \n",
    "#              searchlist=searchlist,\n",
    "#              control = control, \n",
    "#              treatment = treatment, \n",
    "#              explicit=False,\n",
    "#              binContigs = binContigs,\n",
    "#              binRank = binRank,\n",
    "#              min_length = min_length\n",
    "#             )\n",
    "# Q.searchContigs()\n",
    "# Q.makeGTF_and_df()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSubread template script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run RSubread and DESeq2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T16:47:26.517914Z",
     "start_time": "2020-05-06T16:47:26.514431Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:46:06.064038Z",
     "start_time": "2020-05-06T17:46:06.058869Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import argparse\n",
    "def parse_arguments():\n",
    "        parser = argparse.ArgumentParser(description='filter quantify and graph')\n",
    "        parser.add_argument('--col_data', action= 'store', metavar='col_data') \n",
    "#         parser.add_argument('--f', action= 'store', metavar='f') \n",
    "        parser.add_argument('--Nextflow_Out', action= 'store', metavar='--Nextflow_Out') \n",
    "        parser.add_argument('--singleEnd', action= 'store', metavar='--singleEnd') \n",
    "        parser.add_argument('--Nextflow_path', action= 'store', metavar='--Nextflow_path')\n",
    "        parser.add_argument('--Control', action= 'store', metavar='--Control')\n",
    "        parser.add_argument('--Treatment', action= 'store', metavar='--Treatment')\n",
    "        parser.add_argument('--cpus', action= 'store', metavar='--cpus')\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:48:39.301697Z",
     "start_time": "2020-05-06T17:48:39.291789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--col_data col_data]\n",
      "                             [--Nextflow_Out --Nextflow_Out]\n",
      "                             [--singleEnd --singleEnd]\n",
      "                             [--Nextflow_path --Nextflow_path]\n",
      "                             [--Control --Control] [--Treatment --Treatment]\n",
      "                             [--cpus --cpus]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/m/Library/Jupyter/runtime/kernel-9afbd306-675a-4ac3-b94f-7775feb0d421.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "if __name__==\"__main__\":\n",
    "    args = parse_arguments()\n",
    "#     f = args.f\n",
    "    NF_out = args.Nextflow_Out\n",
    "    NF_path = args.Nextflow_path\n",
    "    singleEnd = args.singleEnd\n",
    "    Control = args.Control\n",
    "    Treatment = args.Treatment\n",
    "    cpus = args.cpus\n",
    "\n",
    "    out_path = NF_path\n",
    "    \n",
    "    \n",
    "    \n",
    "    col_data = f\"{NF_path}/sample_table.txt\"\n",
    "    levels = [\"all\",\"dark_all_FG\"]\n",
    "    singleEnd = False\n",
    " \n",
    "\n",
    "\n",
    "    searchlist = None\n",
    "    binRank = \"NA\"\n",
    "    binContigs = False\n",
    "    min_length = 100\n",
    "    title = \"ALL\"\n",
    "    # searchlist = [(True,\"phylum\",\"NA\"), (False,\"phylum\",\"Bacteria_name\")]\n",
    "    Q = Quantify(NF_out=NF_out, \n",
    "                 NF_path=NF_path,\n",
    "                 out_path = out_path, \n",
    "                 col_data = col_data, \n",
    "                 cpus=cpus, \n",
    "                 singleEnd=False, \n",
    "                 title=title, \n",
    "                 searchlist=searchlist,\n",
    "                 control = Control, \n",
    "                 treatment = Treatment, \n",
    "                 explicit=False,\n",
    "                 binContigs = binContigs,\n",
    "                 binRank = binRank,\n",
    "                 min_length = min_length\n",
    "                )\n",
    "    Q.searchContigs()\n",
    "    Q.makeGTF_and_df()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T17:48:40.859266Z",
     "start_time": "2020-05-06T17:48:40.637449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted FilterQuantSubreadDESeq.ipynb to nb_FilterQuantSubreadDESeq.py\r\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "!python notebook2script.py FilterQuantSubreadDESeq.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
